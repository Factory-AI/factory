---
title: Choosing Your Model
description: Balance accuracy, speed, and cost by picking the right model and reasoning level for each CLI task.
---

Model quality evolves quickly, and we tune the CLI defaults as the ecosystem shifts. Use this guide as a snapshot of how the major options compare today, and expect to revisit it as we publish updates. This guide was last updated on Wednesday, September 24th 2025.

---

## 1 · Current stack rank (September 2025)

| Rank | Model                               | Why we reach for it                                                                                                                                           |
| ---- | ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1    | **Claude Sonnet 4.5**               | Recommended daily driver. Stellar for deep planning, architectural reviews, and product specs while keeping latency and cost practical.                   |
| 2    | **Claude Opus 4.1**                 | Comparable depth to Sonnet 4.5 and a dependable fallback when you need every ounce of Anthropic capability for critical audits or extremely long edits.   |
| 3    | **GPT‑5 Codex**                     | Nearly Opus-level output with noticeably lower latency and ~5× lower cost. Excellent for implementation-heavy coding loops.                              |
| 4    | **GPT‑5**                           | Strong generalist. Choose it when you prefer OpenAI ergonomics or require parity with GPT features available elsewhere.                                  |
| 5    | **Claude Haiku 4.5**                | Fast and cost-effective for routine tasks, quick iterations, and high-volume automation.                                                                   |
| 6    | **Droid Core (GLM-4.6)**            | Native Factory model with a 0.25× token multiplier. Lightning-fast and budget-friendly for automation, bulk edits, and environments where you want guaranteed availability without external providers. |

<Note>
  We ship model updates regularly. When a new release overtakes the list above,
  we update this page and the CLI defaults.
</Note>

---

## 2 · Match the model to the job

| Scenario                                                         | Recommended model                                                                                                                                      |
| ---------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Deep planning, architecture reviews, ambiguous product specs** | Default to **Sonnet 4.5** – it delivers Opus-grade reasoning with friendlier turnaround. Pull in **Opus 4.1** only when you need the absolute ceiling.  |
| **Full-feature development, large refactors**                    | **Sonnet 4.5** is the recommended daily driver; switch to **Opus 4.1** for edge cases or **GPT‑5 Codex** when you want faster iteration loops.         |
| **Repeatable edits, summarization, boilerplate generation**      | **GPT‑5** or **Sonnet 4.5** keep costs low while staying accurate.                                                                                     |
| **CI/CD or automation loops**                                    | Favor **Sonnet 4.5** or **GPT‑5 Codex** for predictable throughput. Promote critical planning steps to **Opus 4.1** only when correctness demands it.   |
| **High-volume automation, frequent quick turns**                 | **Claude Haiku 4.5** for speedy feedback loops at the lowest cost tier.                                                                               |

Tip: you can swap models mid-session with `/model` or by toggling in the settings panel (`Shift+Tab` → **Settings**).

---

## 3 · Switching models mid-session

- Use `/model` (or **Shift+Tab → Settings → Model**) to swap without losing your chat history.
- If you change providers (e.g. Anthropc to OpenAI), the CLI converts the session transcript between Anthropic and OpenAI formats. The translation is lossy—provider-specific metadata is dropped—but we have not seen accuracy regressions in practice.
- For the best context continuity, switch models at natural milestones: after a commit, once a PR lands, or when you abandon a failed approach and reset the plan.
- If you flip back and forth rapidly, expect the assistant to spend a turn re-grounding itself; consider summarizing recent progress when you switch.

---

## 4 · Reasoning effort settings

- Anthropic models (Opus/Sonnet) show modest gains between Low and High.
- GPT models respond much more to higher reasoning effort—bumping **GPT‑5** or **GPT‑5 Codex** to **High** can materially improve planning and debugging.
- Reasoning effort increases latency and cost, so start Low for simple work and escalate when you need more depth.

<Tip>
  Change reasoning effort from `/model` → **Reasoning effort**, or via the
  settings menu.
</Tip>

---

## 5 · Bring Your Own Keys (BYOK)

Factory ships with managed Anthropic and OpenAI access. If you prefer to run against your own accounts, BYOK is opt-in—see [Bring Your Own Keys](/cli/configuration/byok) for setup steps, supported providers, and billing notes.

### Open-source models

**Droid Core (GLM-4.6)** is an open-source alternative available in the CLI. It's useful for:

- **Air-gapped environments** where external API calls aren't allowed
- **Cost-sensitive projects** needing unlimited local inference
- **Privacy requirements** where code cannot leave your infrastructure
- **Experimentation** with open-source model capabilities

**Note:** GLM-4.6 does not support image attachments. For image-based workflows, use Claude or GPT models.

To use open-source models, you'll need to configure them via BYOK with a local inference server (like Ollama) or a hosted provider. See [BYOK documentation](/cli/configuration/byok) for setup instructions.

---

## 6 · Keep notes on what works

- Track high-impact workflows (e.g., spec generation vs. quick edits) and which combinations of model + reasoning effort feel best.
- Ping the community or your Factory contact when you notice a model regression so we can benchmark and update this guidance quickly.
